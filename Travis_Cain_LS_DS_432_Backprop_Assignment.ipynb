{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Travis_Cain_LS_DS_432_Backprop_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TravisJRCain/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/Travis_Cain_LS_DS_432_Backprop_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Backpropagation Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
        "\n",
        "Using TensorFlow Keras, Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 0  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 1 |\n",
        "| 0  | 1  | 0  | 1 |\n",
        "| 1  | 0  | 0  | 1 |\n",
        "| 1  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 0  | 0 |\n",
        "\n",
        "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn.\n",
        "\n",
        "This is your \"Hello World!\" of TensorFlow.\n",
        "\n",
        "### Example TensorFlow Starter Code\n",
        "\n",
        "```python \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(3, activation='sigmoid', input_dim=2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "results = model.fit(X,y, epochs=100)\n",
        "\n",
        "```\n",
        "\n",
        "### Additional Written Tasks:\n",
        "1. Investigate the various [loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses). Which is best suited for the task at hand (predicting 1 / 0) and why? \n",
        "2. What is the difference between a loss function and a metric? Why might we need both in Keras? \n",
        "3. Investigate the various [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). Stochastic Gradient Descent (`sgd`) is not the learning algorithm dejour anyone. Why is that? What do newer optimizers such as `adam` have to offer? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEREYT-3wI1f",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# XOR Gate as a numpy array\n",
        "\n",
        "X = np.array([[0, 0, 1],\n",
        "                [0, 1, 1],\n",
        "                [1, 0, 1],\n",
        "                [0, 1, 0],\n",
        "                [1, 0, 0],\n",
        "                [1, 1, 1],\n",
        "                [0, 0, 0]])\n",
        "y = np.array([[0], [1], [1], [1], [1], [0], [0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mwYg7YjaVNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88cb61fb-1625-4917-ae98-2da2d6633fdc"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7, 3), (7, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Vroc0kmu_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9bfd0c0-6467-4093-f417-ff0cdad54ab5"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([Dense(3, activation='relu', input_dim=3),\n",
        "                    Dense(1, activation='sigmoid')])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "results = model.fit(X, y, epochs=100)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8492 - acc: 0.4286\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8476 - acc: 0.2857\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8459 - acc: 0.2857\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8443 - acc: 0.2857\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8427 - acc: 0.2857\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8412 - acc: 0.2857\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8396 - acc: 0.2857\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8381 - acc: 0.4286\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8365 - acc: 0.4286\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8350 - acc: 0.4286\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8335 - acc: 0.4286\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8321 - acc: 0.4286\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8306 - acc: 0.4286\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8292 - acc: 0.4286\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8278 - acc: 0.4286\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8264 - acc: 0.4286\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8250 - acc: 0.4286\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8236 - acc: 0.4286\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8223 - acc: 0.4286\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8209 - acc: 0.4286\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8196 - acc: 0.4286\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8183 - acc: 0.4286\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8170 - acc: 0.4286\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8158 - acc: 0.4286\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8145 - acc: 0.4286\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8132 - acc: 0.4286\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8120 - acc: 0.4286\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8108 - acc: 0.4286\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8096 - acc: 0.4286\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8084 - acc: 0.4286\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8072 - acc: 0.4286\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8061 - acc: 0.4286\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8049 - acc: 0.4286\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8038 - acc: 0.4286\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8026 - acc: 0.4286\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8015 - acc: 0.4286\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8004 - acc: 0.4286\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7993 - acc: 0.4286\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7983 - acc: 0.4286\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7972 - acc: 0.4286\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7962 - acc: 0.4286\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7951 - acc: 0.4286\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7941 - acc: 0.4286\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7931 - acc: 0.5714\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7921 - acc: 0.5714\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7911 - acc: 0.5714\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7901 - acc: 0.5714\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7891 - acc: 0.5714\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7881 - acc: 0.5714\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7872 - acc: 0.5714\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7862 - acc: 0.5714\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7853 - acc: 0.5714\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7843 - acc: 0.5714\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7828 - acc: 0.5714\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7813 - acc: 0.5714\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7798 - acc: 0.5714\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7784 - acc: 0.5714\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7769 - acc: 0.5714\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7755 - acc: 0.5714\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7741 - acc: 0.5714\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7727 - acc: 0.5714\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7713 - acc: 0.5714\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7699 - acc: 0.5714\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7685 - acc: 0.5714\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7672 - acc: 0.5714\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7658 - acc: 0.5714\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7645 - acc: 0.5714\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7631 - acc: 0.5714\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7618 - acc: 0.5714\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7605 - acc: 0.5714\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7592 - acc: 0.5714\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7579 - acc: 0.5714\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7566 - acc: 0.5714\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7554 - acc: 0.5714\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7541 - acc: 0.5714\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7529 - acc: 0.5714\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7516 - acc: 0.5714\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7504 - acc: 0.5714\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7492 - acc: 0.5714\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7480 - acc: 0.5714\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7468 - acc: 0.5714\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7456 - acc: 0.5714\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7444 - acc: 0.5714\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7433 - acc: 0.5714\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7421 - acc: 0.5714\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7410 - acc: 0.5714\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7398 - acc: 0.5714\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7387 - acc: 0.5714\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7376 - acc: 0.5714\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7364 - acc: 0.5714\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7353 - acc: 0.5714\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7342 - acc: 0.5714\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.7331 - acc: 0.5714\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7321 - acc: 0.5714\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7310 - acc: 0.5714\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7299 - acc: 0.5714\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7289 - acc: 0.5714\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7278 - acc: 0.5714\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7268 - acc: 0.5714\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7257 - acc: 0.5714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5CTFyj0pCJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5ec7d79-5c7a-48e5-8f44-bba0706ea4c9"
      },
      "source": [
        "model.predict(np.array([[0,1,0]]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4483308]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Pf16NbopRIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c4961a7b-e27c-4610-a40f-8743feea9c77"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 16\n",
            "Trainable params: 16\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUCW1SJkxYd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "  def __init__(self, inputs, hiddenNodes, outputNodes):\n",
        "    self.inputs = inputs\n",
        "    self.hiddenNodes = hiddenNodes\n",
        "    self.outputNodes = outputNodes\n",
        "\n",
        "\n",
        "# Initialize Weights\n",
        "    self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
        "    self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "  \n",
        "  def sigmoid(self, s):\n",
        "    return 1 / (1+np.exp(-s))\n",
        "\n",
        "  def sigmoidPrime(self, s):\n",
        "    sx = self.sigmoid(s)\n",
        "    return sx * (1-sx)\n",
        "\n",
        "  def feed_forward(self, x):\n",
        "    \"\"\"\n",
        "    Calculates Neural Network\n",
        "    \"\"\"\n",
        "\n",
        "    # weight sum 1\n",
        "    self.hidden_sum = np.dot(X, self.weights1)\n",
        "\n",
        "    # activation\n",
        "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "\n",
        "    # weighted sum 2\n",
        "    self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "\n",
        "    # final ouput\n",
        "    self.activated_output = self.sigmoid(self.output_sum)\n",
        "\n",
        "    return self.activated_output\n",
        "\n",
        "  def backward(self, X,y,o):\n",
        "    \"\"\"\n",
        "    Back prop through network\n",
        "    \"\"\"\n",
        "\n",
        "    self.o_error = y - o\n",
        "\n",
        "    # apply derivate of sigmoid to error\n",
        "    self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)\n",
        "\n",
        "    # calculate how much the ouput layer weights were off\n",
        "    self.z2_error = self.o_delta.dot(self.weights2.T)\n",
        "\n",
        "    # calculate how much the weights were off\n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
        "\n",
        "    self.weights1 += X.T.dot(self.z2_delta)\n",
        "    self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
        "\n",
        "  def train(self, X, y):\n",
        "    o = self.feed_forward(X)\n",
        "    self.backward(X,y,o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTpTL2KA-Myt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86e36cc6-5b36-4f7a-cd1a-df04d754b000"
      },
      "source": [
        "# Neural Network\n",
        "\n",
        "nn = NeuralNetwork(3, 5, 1)\n",
        "\n",
        "for i in range(10000):\n",
        "  if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 == 0):\n",
        "    print('+' + '----' * 2 + f'EPOCH {i+1}' + '-----' * 2 + '+')\n",
        "    print('Input: \\n', X)\n",
        "    print('Actual Output: \\n', y)\n",
        "    print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
        "    print('Loss: \\n', str(np.mean(np.square(y - nn.feed_forward(X)))))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------EPOCH 1----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 2----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 3----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 4----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 5----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 1000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 2000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 3000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 4000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 5000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 6000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 7000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 8000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 9000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n",
            "+--------EPOCH 10000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.29960549]\n",
            " [0.35410082]\n",
            " [0.323638  ]\n",
            " [0.424175  ]\n",
            " [0.38150204]\n",
            " [0.38133139]\n",
            " [0.35743455]]\n",
            "Loss: \n",
            " 0.27881457105304713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5-VuhmXetZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7e3ea0d-3a4d-4aed-d0f7-036b9cbf24c0"
      },
      "source": [
        "# Tensorflow\n",
        "\n",
        "model = Sequential([Dense(2, activation='relu'),\n",
        "                    Dense(3, activation='relu'),\n",
        "                    Dense(4)])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc'])\n",
        "\n",
        "results = model.fit(X,y, epochs=500)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6393 - acc: 0.7143\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6371 - acc: 0.4286\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6350 - acc: 0.5714\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6328 - acc: 0.5714\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6308 - acc: 0.5714\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6287 - acc: 0.5714\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6267 - acc: 0.5714\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6247 - acc: 0.5714\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6227 - acc: 0.5714\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6207 - acc: 0.5714\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6188 - acc: 0.5714\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6168 - acc: 0.5714\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6149 - acc: 0.5714\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6130 - acc: 0.5714\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6111 - acc: 0.5714\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6092 - acc: 0.5714\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6073 - acc: 0.5714\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6055 - acc: 0.5714\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6037 - acc: 0.5714\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6018 - acc: 0.5714\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6000 - acc: 0.5714\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5983 - acc: 0.5714\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5965 - acc: 0.5714\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5947 - acc: 0.5714\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5930 - acc: 0.5714\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5913 - acc: 0.5714\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5896 - acc: 0.5714\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5879 - acc: 0.5714\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5862 - acc: 0.5714\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5845 - acc: 0.5714\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5829 - acc: 0.5714\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5813 - acc: 0.5714\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5797 - acc: 0.5714\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5781 - acc: 0.5714\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5765 - acc: 0.5714\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5750 - acc: 0.5714\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5734 - acc: 0.5714\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5718 - acc: 0.5714\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5703 - acc: 0.5714\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5687 - acc: 0.5714\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5672 - acc: 0.5714\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5657 - acc: 0.5714\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5642 - acc: 0.5714\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5627 - acc: 0.5714\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5612 - acc: 0.5714\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5597 - acc: 0.5714\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5582 - acc: 0.5714\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5567 - acc: 0.5714\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5553 - acc: 0.5714\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5538 - acc: 0.5714\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5524 - acc: 0.5714\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5509 - acc: 0.5714\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5495 - acc: 0.5714\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5480 - acc: 0.5714\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5466 - acc: 0.5714\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5452 - acc: 0.5714\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5438 - acc: 0.5714\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5424 - acc: 0.5714\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5410 - acc: 0.5714\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5396 - acc: 0.5714\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5382 - acc: 0.5714\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.5369 - acc: 0.5714\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5355 - acc: 0.5714\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5341 - acc: 0.5714\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5328 - acc: 0.5714\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5314 - acc: 0.5714\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5301 - acc: 0.5714\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5288 - acc: 0.5714\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5274 - acc: 0.5714\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5261 - acc: 0.5714\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5248 - acc: 0.5714\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5235 - acc: 0.5714\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5222 - acc: 0.5714\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5209 - acc: 0.5714\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5196 - acc: 0.5714\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5183 - acc: 0.5714\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5171 - acc: 0.5714\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5158 - acc: 0.5714\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5145 - acc: 0.5714\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5133 - acc: 0.5714\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5120 - acc: 0.5714\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5108 - acc: 0.5714\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5095 - acc: 0.5714\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5083 - acc: 0.5714\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5071 - acc: 0.5714\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5059 - acc: 0.5714\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5046 - acc: 0.5714\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5034 - acc: 0.5714\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5022 - acc: 0.5714\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5010 - acc: 0.5714\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4998 - acc: 0.5714\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4987 - acc: 0.5714\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4975 - acc: 0.5714\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4963 - acc: 0.5714\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4951 - acc: 0.5714\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4940 - acc: 0.5714\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4928 - acc: 0.5714\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4917 - acc: 0.5714\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4905 - acc: 0.5714\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4894 - acc: 0.5714\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4882 - acc: 0.5714\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4871 - acc: 0.5714\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4860 - acc: 0.5714\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4849 - acc: 0.5714\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4838 - acc: 0.5714\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4826 - acc: 0.5714\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4815 - acc: 0.5714\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4804 - acc: 0.5714\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4793 - acc: 0.5714\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4783 - acc: 0.5714\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4772 - acc: 0.5714\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4761 - acc: 0.5714\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4750 - acc: 0.5714\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4740 - acc: 0.5714\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4729 - acc: 0.5714\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4718 - acc: 0.5714\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4708 - acc: 0.5714\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4697 - acc: 0.5714\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4687 - acc: 0.5714\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4677 - acc: 0.5714\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4666 - acc: 0.5714\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4656 - acc: 0.5714\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4646 - acc: 0.5714\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4635 - acc: 0.5714\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4625 - acc: 0.5714\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4615 - acc: 0.5714\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4605 - acc: 0.5714\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4595 - acc: 0.5714\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4585 - acc: 0.5714\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.4575 - acc: 0.5714\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4565 - acc: 0.5714\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4556 - acc: 0.5714\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4546 - acc: 0.5714\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4536 - acc: 0.5714\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4526 - acc: 0.5714\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4517 - acc: 0.5714\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4507 - acc: 0.5714\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4498 - acc: 0.5714\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4488 - acc: 0.5714\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.4479 - acc: 0.5714\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4469 - acc: 0.5714\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4460 - acc: 0.5714\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4450 - acc: 0.5714\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4441 - acc: 0.5714\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4432 - acc: 0.5714\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4423 - acc: 0.5714\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4413 - acc: 0.5714\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4404 - acc: 0.5714\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4395 - acc: 0.5714\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4386 - acc: 0.5714\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4377 - acc: 0.5714\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4368 - acc: 0.5714\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4359 - acc: 0.5714\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4350 - acc: 0.5714\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4342 - acc: 0.5714\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4333 - acc: 0.5714\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4324 - acc: 0.5714\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4315 - acc: 0.5714\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4306 - acc: 0.5714\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.4298 - acc: 0.5714\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4289 - acc: 0.5714\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4281 - acc: 0.5714\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4272 - acc: 0.5714\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4264 - acc: 0.5714\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4255 - acc: 0.5714\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4247 - acc: 0.5714\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4238 - acc: 0.5714\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4230 - acc: 0.5714\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4222 - acc: 0.5714\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4213 - acc: 0.5714\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4205 - acc: 0.5714\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4197 - acc: 0.5714\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4189 - acc: 0.5714\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4181 - acc: 0.5714\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4172 - acc: 0.5714\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4164 - acc: 0.5714\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4156 - acc: 0.5714\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4148 - acc: 0.5714\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4140 - acc: 0.5714\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4132 - acc: 0.5714\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4125 - acc: 0.5714\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4117 - acc: 0.5714\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4109 - acc: 0.5714\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4101 - acc: 0.5714\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4093 - acc: 0.5714\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4086 - acc: 0.5714\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4078 - acc: 0.5714\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4070 - acc: 0.5714\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4063 - acc: 0.5714\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4055 - acc: 0.5714\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4047 - acc: 0.5714\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4040 - acc: 0.5714\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4032 - acc: 0.5714\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4025 - acc: 0.5714\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4017 - acc: 0.5714\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4010 - acc: 0.5714\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4003 - acc: 0.5714\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3995 - acc: 0.5714\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3988 - acc: 0.5714\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3981 - acc: 0.5714\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3973 - acc: 0.5714\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3966 - acc: 0.5714\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3959 - acc: 0.5714\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3952 - acc: 0.5714\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3945 - acc: 0.5714\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3938 - acc: 0.5714\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3931 - acc: 0.5714\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3924 - acc: 0.5714\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3917 - acc: 0.5714\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3910 - acc: 0.5714\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3903 - acc: 0.5714\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3896 - acc: 0.5714\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3889 - acc: 0.5714\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3882 - acc: 0.5714\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3875 - acc: 0.5714\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3868 - acc: 0.5714\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3862 - acc: 0.5714\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3855 - acc: 0.5714\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3848 - acc: 0.5714\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3841 - acc: 0.5714\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3835 - acc: 0.5714\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3828 - acc: 0.5714\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3822 - acc: 0.5714\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3815 - acc: 0.5714\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3808 - acc: 0.5714\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3802 - acc: 0.5714\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3795 - acc: 0.5714\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3789 - acc: 0.5714\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3783 - acc: 0.5714\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3776 - acc: 0.5714\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3770 - acc: 0.5714\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3763 - acc: 0.5714\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3757 - acc: 0.5714\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3751 - acc: 0.5714\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3744 - acc: 0.5714\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3738 - acc: 0.5714\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3732 - acc: 0.5714\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3726 - acc: 0.5714\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3720 - acc: 0.5714\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3713 - acc: 0.5714\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3707 - acc: 0.5714\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3701 - acc: 0.5714\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3695 - acc: 0.5714\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3689 - acc: 0.5714\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3683 - acc: 0.5714\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3677 - acc: 0.5714\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3671 - acc: 0.5714\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 944us/step - loss: 0.3665 - acc: 0.5714\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 955us/step - loss: 0.3659 - acc: 0.5714\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 889us/step - loss: 0.3653 - acc: 0.5714\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3647 - acc: 0.5714\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 936us/step - loss: 0.3642 - acc: 0.5714\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3636 - acc: 0.5714\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3630 - acc: 0.5714\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3624 - acc: 0.5714\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3618 - acc: 0.5714\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3613 - acc: 0.5714\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3607 - acc: 0.5714\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3601 - acc: 0.5714\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3596 - acc: 0.5714\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3590 - acc: 0.5714\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3584 - acc: 0.5714\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3579 - acc: 0.5714\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3573 - acc: 0.5714\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3568 - acc: 0.5714\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3562 - acc: 0.5714\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3557 - acc: 0.5714\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3551 - acc: 0.5714\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3546 - acc: 0.5714\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3540 - acc: 0.5714\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3535 - acc: 0.5714\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3529 - acc: 0.5714\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3524 - acc: 0.5714\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3519 - acc: 0.5714\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3513 - acc: 0.5714\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3508 - acc: 0.5714\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3503 - acc: 0.5714\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3498 - acc: 0.5714\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3492 - acc: 0.5714\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3487 - acc: 0.5714\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3482 - acc: 0.5714\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3477 - acc: 0.5714\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3472 - acc: 0.5714\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3467 - acc: 0.5714\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3461 - acc: 0.5714\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3456 - acc: 0.5714\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3451 - acc: 0.5714\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3446 - acc: 0.5714\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3441 - acc: 0.5714\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3436 - acc: 0.5714\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3431 - acc: 0.5714\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3426 - acc: 0.5714\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3421 - acc: 0.5714\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3416 - acc: 0.5714\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3411 - acc: 0.5714\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3407 - acc: 0.5714\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3402 - acc: 0.5714\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3397 - acc: 0.5714\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3392 - acc: 0.5714\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3387 - acc: 0.5714\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3382 - acc: 0.5714\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3378 - acc: 0.5714\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3373 - acc: 0.5714\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3368 - acc: 0.5714\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3364 - acc: 0.5714\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3359 - acc: 0.5714\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3354 - acc: 0.5714\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3350 - acc: 0.5714\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3345 - acc: 0.5714\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3340 - acc: 0.5714\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3336 - acc: 0.5714\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3331 - acc: 0.5714\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3327 - acc: 0.5714\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3322 - acc: 0.5714\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3318 - acc: 0.5714\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3313 - acc: 0.5714\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3309 - acc: 0.5714\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3304 - acc: 0.5714\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3300 - acc: 0.5714\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3295 - acc: 0.5714\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3291 - acc: 0.5714\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3286 - acc: 0.5714\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3282 - acc: 0.5714\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3278 - acc: 0.5714\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3273 - acc: 0.5714\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3269 - acc: 0.5714\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3265 - acc: 0.5714\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3261 - acc: 0.5714\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3256 - acc: 0.5714\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3252 - acc: 0.5714\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3248 - acc: 0.5714\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3244 - acc: 0.5714\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3239 - acc: 0.5714\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3235 - acc: 0.5714\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3231 - acc: 0.5714\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3227 - acc: 0.5714\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3223 - acc: 0.5714\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3219 - acc: 0.5714\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3215 - acc: 0.5714\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3211 - acc: 0.5714\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3206 - acc: 0.5714\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3202 - acc: 0.5714\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3198 - acc: 0.5714\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3194 - acc: 0.5714\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3190 - acc: 0.5714\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3186 - acc: 0.5714\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3183 - acc: 0.5714\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3179 - acc: 0.5714\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3175 - acc: 0.5714\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3171 - acc: 0.5714\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3167 - acc: 0.5714\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3163 - acc: 0.5714\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3159 - acc: 0.5714\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3155 - acc: 0.5714\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3151 - acc: 0.5714\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3148 - acc: 0.5714\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3144 - acc: 0.5714\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3140 - acc: 0.5714\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3136 - acc: 0.5714\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3133 - acc: 0.5714\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3129 - acc: 0.5714\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3125 - acc: 0.5714\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3121 - acc: 0.5714\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3118 - acc: 0.5714\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3114 - acc: 0.5714\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3110 - acc: 0.5714\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3107 - acc: 0.5714\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3103 - acc: 0.5714\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3100 - acc: 0.5714\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3096 - acc: 0.5714\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3092 - acc: 0.5714\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3089 - acc: 0.5714\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3085 - acc: 0.5714\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3082 - acc: 0.5714\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3078 - acc: 0.5714\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3075 - acc: 0.5714\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3071 - acc: 0.5714\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3068 - acc: 0.5714\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3064 - acc: 0.5714\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3061 - acc: 0.5714\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3057 - acc: 0.5714\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3054 - acc: 0.5714\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3051 - acc: 0.5714\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3047 - acc: 0.5714\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3044 - acc: 0.5714\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3040 - acc: 0.5714\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3037 - acc: 0.5714\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3034 - acc: 0.5714\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3030 - acc: 0.5714\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3027 - acc: 0.5714\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3024 - acc: 0.5714\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3021 - acc: 0.5714\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3017 - acc: 0.5714\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3014 - acc: 0.5714\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3011 - acc: 0.5714\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3008 - acc: 0.5714\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3004 - acc: 0.5714\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3001 - acc: 0.5714\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2998 - acc: 0.5714\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2995 - acc: 0.5714\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2992 - acc: 0.5714\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2989 - acc: 0.5714\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2985 - acc: 0.5714\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2982 - acc: 0.5714\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2979 - acc: 0.5714\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2976 - acc: 0.5714\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2973 - acc: 0.5714\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2970 - acc: 0.5714\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2967 - acc: 0.5714\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2964 - acc: 0.5714\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2961 - acc: 0.5714\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2958 - acc: 0.5714\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2955 - acc: 0.5714\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2952 - acc: 0.5714\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2949 - acc: 0.5714\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2946 - acc: 0.5714\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2943 - acc: 0.5714\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2940 - acc: 0.5714\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2937 - acc: 0.5714\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2934 - acc: 0.5714\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2931 - acc: 0.5714\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2929 - acc: 0.5714\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2926 - acc: 0.5714\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2923 - acc: 0.5714\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2920 - acc: 0.5714\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2917 - acc: 0.5714\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 936us/step - loss: 0.2914 - acc: 0.5714\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 938us/step - loss: 0.2912 - acc: 0.5714\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2909 - acc: 0.5714\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2906 - acc: 0.5714\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2903 - acc: 0.5714\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2900 - acc: 0.5714\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2898 - acc: 0.5714\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2895 - acc: 0.5714\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2892 - acc: 0.5714\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2890 - acc: 0.5714\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2887 - acc: 0.5714\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2884 - acc: 0.5714\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2882 - acc: 0.5714\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2879 - acc: 0.5714\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2876 - acc: 0.5714\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2874 - acc: 0.5714\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2871 - acc: 0.5714\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2868 - acc: 0.5714\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2866 - acc: 0.5714\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2863 - acc: 0.5714\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2861 - acc: 0.5714\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2858 - acc: 0.5714\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2855 - acc: 0.5714\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2853 - acc: 0.5714\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2850 - acc: 0.5714\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2848 - acc: 0.5714\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2845 - acc: 0.5714\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2843 - acc: 0.5714\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2840 - acc: 0.5714\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2838 - acc: 0.5714\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2835 - acc: 0.5714\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2833 - acc: 0.5714\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2830 - acc: 0.5714\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2828 - acc: 0.5714\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2826 - acc: 0.5714\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2823 - acc: 0.5714\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2821 - acc: 0.5714\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2818 - acc: 0.5714\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2816 - acc: 0.5714\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2814 - acc: 0.5714\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2811 - acc: 0.5714\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2809 - acc: 0.5714\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2807 - acc: 0.5714\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2804 - acc: 0.5714\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2802 - acc: 0.5714\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2800 - acc: 0.5714\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2797 - acc: 0.5714\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2795 - acc: 0.5714\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2793 - acc: 0.5714\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2790 - acc: 0.5714\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2788 - acc: 0.5714\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2786 - acc: 0.5714\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2784 - acc: 0.5714\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2781 - acc: 0.5714\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2779 - acc: 0.5714\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2777 - acc: 0.5714\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2775 - acc: 0.5714\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2773 - acc: 0.5714\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2770 - acc: 0.5714\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2768 - acc: 0.5714\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2766 - acc: 0.5714\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2764 - acc: 0.5714\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2762 - acc: 0.5714\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2760 - acc: 0.5714\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2758 - acc: 0.5714\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2756 - acc: 0.5714\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2753 - acc: 0.5714\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2751 - acc: 0.5714\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2749 - acc: 0.5714\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2747 - acc: 0.5714\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2745 - acc: 0.5714\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2743 - acc: 0.5714\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2741 - acc: 0.5714\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2739 - acc: 0.5714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNQvrDiFfRpA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac9ebcd6-671f-41ca-c26e-392449254aa7"
      },
      "source": [
        "model.predict(np.array([[0,1,1]]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38465953, 0.38574266, 0.38270795, 0.38141838]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sD6Cuu3fXES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "01c9d38b-ea7a-44b8-acac-8da622f9070b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              multiple                  8         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  9         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              multiple                  16        \n",
            "=================================================================\n",
            "Total params: 33\n",
            "Trainable params: 33\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDSIvfTbaRjG",
        "colab_type": "text"
      },
      "source": [
        "### Build a Tensor Keras Perceptron\n",
        "\n",
        "Try to match the architecture we used on Monday - inputs nodes and one output node. Apply this architecture to the XOR-ish dataset above. \n",
        "\n",
        "After fitting your model answer these questions: \n",
        "\n",
        "Are you able to achieve the same results as a bigger architecture from the first part of the assignment? Why is this disparity the case? What properties of the XOR dataset would cause this disparity? \n",
        "\n",
        "Now extrapolate this behavior on a much larger dataset in terms of features. What kind of architecture decisions could we make to avoid the problems the XOR dataset presents at scale? \n",
        "\n",
        "*Note:* The bias term is baked in by default in the Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADZi89IQaRjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Compare "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8b-r70o8p2Dm"
      },
      "source": [
        "## Try building/training a more complex MLP on a bigger dataset.\n",
        "\n",
        "Use TensorFlow Keras & the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the canonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
        "\n",
        "If you need inspiration, the Internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n",
        "\n",
        "\n",
        "### Parts\n",
        "1. Gathering & Transforming the Data\n",
        "2. Making MNIST a Binary Problem\n",
        "3. Estimating your Neural Network (the part you focus on)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-87b3LgXaRjM",
        "colab_type": "text"
      },
      "source": [
        "### Gathering the Data \n",
        "\n",
        "`keras` has a handy method to pull the mnist dataset for you. You'll notice that each observation is a 28x28 arrary which represents an image. Although most Neural Network frameworks can handle higher dimensional data, that is more overhead than necessary for us. We need to flatten the image to one long row which will be 784 values (28X28). Basically, you will be appending each row to one another to make on really long row. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK0pstPiaRjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKgESTFGaRjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibRZ_my7aRjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8AZla6oaRjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
        "\n",
        "# Normalize Our Data\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4_P7yvRaRjt",
        "colab_type": "code",
        "outputId": "550ebce1-0569-4f8a-dd22-c2938737d828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Now the data should be in a format you're more familiar with\n",
        "x_train.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8ZfP6gLaRj0",
        "colab_type": "text"
      },
      "source": [
        "### Making MNIST a Binary Problem \n",
        "MNIST is multiclass classification problem; however we haven't covered all the necessary techniques to handle this yet. You would need to one-hot encode the target, use a different loss metric, and use softmax activations for the last layer. This is all stuff we'll cover later this week, but let us simplify the problem for now: Zero or all else."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iiQNOZWaRj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_temp = np.zeros(y_train.shape)\n",
        "y_temp[np.where(y_train == 0.0)[0]] = 1\n",
        "y_train = y_temp\n",
        "\n",
        "y_temp = np.zeros(y_test.shape)\n",
        "y_temp[np.where(y_test == 0.0)[0]] = 1\n",
        "y_test = y_temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zlsRRcBaRj6",
        "colab_type": "code",
        "outputId": "b692d9e4-00ff-4f9a-e737-3622dfcf9df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# A Nice Binary target for ya to work with\n",
        "y_train"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGfvj8_paRj9",
        "colab_type": "text"
      },
      "source": [
        "### Estimating Your `net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5MOPtYdk1HgA",
        "colab": {}
      },
      "source": [
        "\n",
        "##### Your Code Here #####\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # Other self variables (dataset shape, etc)\n",
        "        self.inputs = 784\n",
        "        self.hiddenNodes = 500\n",
        "        self.outputNodes = 1\n",
        "        \n",
        "        # Initial weights 784*500, input to hidden\n",
        "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
        "        self.weights1 = self.weights1 / self.weights1.shape[0]\n",
        "        \n",
        "        # 500 * 1 (second set), hidden to output\n",
        "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "        self.weights2 = self.weights2 / self.weights2.shape[0]\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + np.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        ss = self.sigmoid(s)\n",
        "        return ss * (1 - ss)\n",
        "    \n",
        "    def feed_forward(self, X):\n",
        "        \n",
        "        # Weighted sum\n",
        "        self.hidden_sum = np.dot(X, self.weights1)\n",
        "        \n",
        "        # Activate\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        \n",
        "        # Weighted sum of activated hidden (the output layer)\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "        \n",
        "        # Final activation of output (prediction)\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "\n",
        "        return self.activated_output\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        # Backprop through network\n",
        "        \n",
        "        self.o_error = y - o # error in output\n",
        "        \n",
        "        # apply derivative of sigmoid to error\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
        "        \n",
        "        # z2 error: how much our output layer weight was off\n",
        "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
        "        \n",
        "        # z2 delta: how much were the weights off by?\n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
        "        \n",
        "        # adjust first set (input -> hidden) weights\n",
        "        self.weights1 += self.learning_rate * np.reshape(X, (-1, 1)).dot(np.reshape(self.z2_delta, (-1, 1)).T)\n",
        "        \n",
        "        # adjust second set\n",
        "        self.weights2 += self.learning_rate * np.reshape(self.activated_hidden, (-1, 1)).dot(np.reshape(self.o_delta, (-1, 1)).T)\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aSfZeT5fzvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce51fefe-bffc-40bb-bef8-ed4065e6c01e"
      },
      "source": [
        "\n",
        "# Let's train it.\n",
        "\n",
        "learning_rate = 0.1\n",
        "nn = NeuralNetwork(learning_rate)\n",
        "epochs = 500\n",
        "batch_size = 100\n",
        "\n",
        "# Number of epochs/iterations.\n",
        "losses = []\n",
        "for i in range(epochs):\n",
        "    loss = 0\n",
        "    for k in range(batch_size):\n",
        "        col = np.random.randint(x_train.shape[0])\n",
        "        input_vector = x_train[col, :]\n",
        "        target_vector = y_train[col]\n",
        "        nn.train(input_vector, target_vector)\n",
        "        loss += np.linalg.norm(nn.sigmoid(np.dot(nn.weights2.T, nn.sigmoid(np.dot(nn.weights1.T, input_vector)))) - target_vector, 2)\n",
        "    if (i+1) % 10 == 0:\n",
        "        print('+' + '----' * 2 + f'EPOCH {i + 1}' + '----' * 2 + '+')\n",
        "        print(\"Loss: \\n\", str(loss/batch_size))\n",
        "    losses.append(loss)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------EPOCH 10--------+\n",
            "Loss: \n",
            " 0.13881489952141846\n",
            "+--------EPOCH 20--------+\n",
            "Loss: \n",
            " 0.06019849254763906\n",
            "+--------EPOCH 30--------+\n",
            "Loss: \n",
            " 0.042340292584468975\n",
            "+--------EPOCH 40--------+\n",
            "Loss: \n",
            " 0.021400982512368036\n",
            "+--------EPOCH 50--------+\n",
            "Loss: \n",
            " 0.014041478231059763\n",
            "+--------EPOCH 60--------+\n",
            "Loss: \n",
            " 0.018295994769069453\n",
            "+--------EPOCH 70--------+\n",
            "Loss: \n",
            " 0.030682627607738943\n",
            "+--------EPOCH 80--------+\n",
            "Loss: \n",
            " 0.02108084980897987\n",
            "+--------EPOCH 90--------+\n",
            "Loss: \n",
            " 0.034236525796908876\n",
            "+--------EPOCH 100--------+\n",
            "Loss: \n",
            " 0.009260321571712155\n",
            "+--------EPOCH 110--------+\n",
            "Loss: \n",
            " 0.011997118934386077\n",
            "+--------EPOCH 120--------+\n",
            "Loss: \n",
            " 0.004862272891549111\n",
            "+--------EPOCH 130--------+\n",
            "Loss: \n",
            " 0.009562370727166102\n",
            "+--------EPOCH 140--------+\n",
            "Loss: \n",
            " 0.011717642066912353\n",
            "+--------EPOCH 150--------+\n",
            "Loss: \n",
            " 0.007459638974589761\n",
            "+--------EPOCH 160--------+\n",
            "Loss: \n",
            " 0.0050563347369437395\n",
            "+--------EPOCH 170--------+\n",
            "Loss: \n",
            " 0.007616643028189925\n",
            "+--------EPOCH 180--------+\n",
            "Loss: \n",
            " 0.014249416128341352\n",
            "+--------EPOCH 190--------+\n",
            "Loss: \n",
            " 0.008919217321222904\n",
            "+--------EPOCH 200--------+\n",
            "Loss: \n",
            " 0.005312411697178757\n",
            "+--------EPOCH 210--------+\n",
            "Loss: \n",
            " 0.008086142110186436\n",
            "+--------EPOCH 220--------+\n",
            "Loss: \n",
            " 0.006175372646299296\n",
            "+--------EPOCH 230--------+\n",
            "Loss: \n",
            " 0.009434866212530287\n",
            "+--------EPOCH 240--------+\n",
            "Loss: \n",
            " 0.029735361760259864\n",
            "+--------EPOCH 250--------+\n",
            "Loss: \n",
            " 0.007211067954734338\n",
            "+--------EPOCH 260--------+\n",
            "Loss: \n",
            " 0.012973302438880605\n",
            "+--------EPOCH 270--------+\n",
            "Loss: \n",
            " 0.009398635270360878\n",
            "+--------EPOCH 280--------+\n",
            "Loss: \n",
            " 0.004306273903122506\n",
            "+--------EPOCH 290--------+\n",
            "Loss: \n",
            " 0.008706333503483286\n",
            "+--------EPOCH 300--------+\n",
            "Loss: \n",
            " 0.008110800096616449\n",
            "+--------EPOCH 310--------+\n",
            "Loss: \n",
            " 0.008522496929715325\n",
            "+--------EPOCH 320--------+\n",
            "Loss: \n",
            " 0.006803888773225455\n",
            "+--------EPOCH 330--------+\n",
            "Loss: \n",
            " 0.002499693936053339\n",
            "+--------EPOCH 340--------+\n",
            "Loss: \n",
            " 0.021913213044360738\n",
            "+--------EPOCH 350--------+\n",
            "Loss: \n",
            " 0.006295523548948641\n",
            "+--------EPOCH 360--------+\n",
            "Loss: \n",
            " 0.011645034700149043\n",
            "+--------EPOCH 370--------+\n",
            "Loss: \n",
            " 0.016476689902084993\n",
            "+--------EPOCH 380--------+\n",
            "Loss: \n",
            " 0.009505065591528335\n",
            "+--------EPOCH 390--------+\n",
            "Loss: \n",
            " 0.009681626154563732\n",
            "+--------EPOCH 400--------+\n",
            "Loss: \n",
            " 0.01336970983713536\n",
            "+--------EPOCH 410--------+\n",
            "Loss: \n",
            " 0.003131666478363615\n",
            "+--------EPOCH 420--------+\n",
            "Loss: \n",
            " 0.002676557566235306\n",
            "+--------EPOCH 430--------+\n",
            "Loss: \n",
            " 0.008545953386244106\n",
            "+--------EPOCH 440--------+\n",
            "Loss: \n",
            " 0.006700212085783468\n",
            "+--------EPOCH 450--------+\n",
            "Loss: \n",
            " 0.020663932429584718\n",
            "+--------EPOCH 460--------+\n",
            "Loss: \n",
            " 0.016631225017397472\n",
            "+--------EPOCH 470--------+\n",
            "Loss: \n",
            " 0.006748900043244385\n",
            "+--------EPOCH 480--------+\n",
            "Loss: \n",
            " 0.008292370261610598\n",
            "+--------EPOCH 490--------+\n",
            "Loss: \n",
            " 0.0062828592677894\n",
            "+--------EPOCH 500--------+\n",
            "Loss: \n",
            " 0.008770680768487681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwlRJSfBlCvy"
      },
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Make MNIST a multiclass problem using cross entropy & soft-max\n",
        "- Implement Cross Validation model evaluation on your MNIST implementation \n",
        "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
        " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
        "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
      ]
    }
  ]
}